{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright **`(c)`** 2023 Giovanni Squillero `<giovanni.squillero@polito.it>`  \n",
    "[`https://github.com/squillero/computational-intelligence`](https://github.com/squillero/computational-intelligence)  \n",
    "Free for personal or classroom use; see [`LICENSE.md`](https://github.com/squillero/computational-intelligence/blob/master/LICENSE.md) for details.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LAB10\n",
    "\n",
    "Use reinforcement learning to devise a tic-tac-toe player.\n",
    "\n",
    "### Deadlines:\n",
    "\n",
    "* Submission: [Dies Natalis Solis Invicti](https://en.wikipedia.org/wiki/Sol_Invictus)\n",
    "* Reviews: [Befana](https://en.wikipedia.org/wiki/Befana)\n",
    "\n",
    "Notes:\n",
    "\n",
    "* Reviews will be assigned  on Monday, December 4\n",
    "* You need to commit in order to be selected as a reviewer (ie. better to commit an empty work than not to commit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice\n",
    "from itertools import permutations\n",
    "from more_itertools import unique_everseen\n",
    "from copy import deepcopy\n",
    "from numpy import base_repr\n",
    "from enum import Enum\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can model a Tic Tac Toe board as a magic square\n",
    "\n",
    "| 2 | 9 | 4 |\n",
    "|---|---|---|\n",
    "| **7** | **5** | **3** |\n",
    "| **6** | **1** | **8** |\n",
    "\n",
    "Using this model, position **2** corresponds to the top left-square on the board.  \n",
    "To win, a player must choose positions whose sum equals 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAGIC = [2, 9, 4, 7, 5, 3, 6, 1, 8]\n",
    "\n",
    "class Status(Enum):\n",
    "    ONGOING = 0\n",
    "    X_WINS = 1\n",
    "    O_WINS = 2\n",
    "    DRAW = 3\n",
    "    INVALID = 4\n",
    "\n",
    "class Player(Enum):\n",
    "    NONE = 0\n",
    "    X = 1\n",
    "    O = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start by defining a `TicTacToe` class, for representing the Tic Tac Toe board.\n",
    "It has the following attributes:\n",
    "- `x` is a set representing the moves played by X\n",
    "- `o` is a set representing the moves played by O\n",
    "- `available` is a set representing the moves that are still available\n",
    "\n",
    "It implements the following methods:\n",
    "- `__init__`: Initializes the board.  \n",
    "It can accept the following arguments:\n",
    "    - If a list of distinct integers between 1 and 9 is passed as an argument, the board is initialized as an empty board with the moves defined by the list played in order\n",
    "    - If a single integer is passed as an argument, the board is initialized based on the ternary representation of the argument (see `__int__` for more info)\n",
    "    - If no arguments are passed, the board is empty.\n",
    "- `__int__`: Converts the board into an integer representation.\n",
    "    - Each position of the board is mapped to a digit of a 9-trit ternary number\n",
    "        - `0` are empty spaces, `1` are Xs, `2` are Os\n",
    "    - The ternary number is then converted to decimal  \n",
    "    Returns the decimal integer used to represent the board.\n",
    "- `__str__`: Converts the board to a string, useful for displaying.  \n",
    "    Returns the string used to represent the board.\n",
    "- `check_status`: Checks if any player won  \n",
    "    Returns a `Status`: \n",
    "    - `Status.ONGOING` if no player won and there are still moves available\n",
    "    - `Status.X_WINS` if x won\n",
    "    - `Status.O_WINS` if o won\n",
    "    - `Status.DRAW` if no player won and there are no more moves available\n",
    "    - `Status.INVALID` if both players won\n",
    "- `play`: Plays a move.  \n",
    "    Argument:\n",
    "    - `pos`: An integer representing the position where to play the move. If the position is not available, the move is discarded.  \n",
    "Returns `True` if the move was played, `False` otherwise.\n",
    "- `transform`: Applies a sequence of transformations. Accepts the following arguments:\n",
    "    - `sequence`: the sequence of transformations to apply. A transformation is defined as a dictionary where the keys are the positions before the transformation and the values are those after\n",
    "    - `revert` (default `False`): if `True`, the transformations are reversed and applied in reverse order  \n",
    "    Returns the transformed board.\n",
    "- `canonize`: Transforms the board into a canonical form\n",
    "    - A sequence of transformations (90Â° clockwise rotation and vertical flip) is applied to the board, then each transformed board is mapped to its integer representation.\n",
    "    - The one with the smallest integer representation is chosen. \n",
    "    Returns the canonical representation and the sequence of transformations used to reach it.\n",
    "- `valid`: Checks if the board is valid\n",
    "    - The board is considered valid if there is at most one more x than o and at most one player won  \n",
    "    Returns `True` of the board is considered valid, `False` otherwise.\n",
    "- `current_player`: Calculates the player who has yet to take his turn  \n",
    "    Returns a `Player`:\n",
    "    - `Player.X` if it's x's turn\n",
    "    - `Player.O` if it's o's turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TicTacToe:\n",
    "\n",
    "\n",
    "    def __init__(self, *args) -> None:\n",
    "        \"\"\"Initialize a Tic-Tac-Toe board. If initialized using a list of integers, returns a board where the moves in the list are played in order. If initialized using a single integer, returns the board based on its unique representation (see __int__). Otherwise initializes an empty board.\"\"\"\n",
    "        if len(args) > 0 and isinstance(args[0], list):\n",
    "            moves = args[0]\n",
    "            self.x = {e for i, e in enumerate(moves) if i%2==0}\n",
    "            self.o = {e for i, e in enumerate(moves) if i%2==1}\n",
    "            self.available = {e for e in range(1, 10) if e not in moves}\n",
    "        elif len(args) > 0 and isinstance(args[0], int):\n",
    "            num = base_repr(args[0], 3).rjust(9, '0')  # ternary representation of the board, with leading zeros to length 9\n",
    "            self.available = {9 - i for i, e in enumerate(num) if e=='0'}     \n",
    "            self.x = {9 - i for i, e in enumerate(num) if e=='1'}\n",
    "            self.o = {9 - i for i, e in enumerate(num) if e=='2'}\n",
    "        else:\n",
    "            self.x = set()\n",
    "            self.o = set()\n",
    "            self.available = set(range(1, 10))\n",
    "\n",
    "\n",
    "    def __int__(self) -> int:\n",
    "        \"\"\"Represents the state of the board as a unique number\"\"\"\n",
    "        res = ''\n",
    "        for element in range(9, 0, -1):\n",
    "            if element in self.x:\n",
    "                res += '1'\n",
    "            elif element in self.o:\n",
    "                res += '2'\n",
    "            else:\n",
    "                res += '0'\n",
    "        return int(res, 3)\n",
    "\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        \"\"\"Returns a string used to display the board state\"\"\"\n",
    "        res = \"\"\n",
    "        for r in range(3):\n",
    "            for c in range(3):\n",
    "                i = 3 * r + c\n",
    "                if MAGIC[i] in self.x:\n",
    "                    res += \"x\"\n",
    "                elif MAGIC[i] in self.o:\n",
    "                    res += \"o\"\n",
    "                else:\n",
    "                    res += \"-\"\n",
    "            res += \"\\n\"\n",
    "        return res\n",
    "\n",
    "\n",
    "    def check_status(self):\n",
    "        \"\"\"Check if the game is terminated. A game is terminated if either player won, no more moves are available, or the state is invalid.\"\"\"\n",
    "        curr_status = Status.ONGOING\n",
    "        for t in permutations(self.x, 3):\n",
    "            if sum(t) == 15:\n",
    "                if curr_status==Status.ONGOING or curr_status==Status.X_WINS:\n",
    "                    curr_status = Status.X_WINS\n",
    "                else:\n",
    "                    curr_status = Status.INVALID\n",
    "                break\n",
    "        for t in permutations(self.o, 3):\n",
    "            if sum(t) == 15:\n",
    "                if curr_status==Status.ONGOING:\n",
    "                    curr_status = Status.O_WINS\n",
    "                else:\n",
    "                    curr_status = Status.INVALID\n",
    "                break\n",
    "        if curr_status == Status.ONGOING and not self.available:\n",
    "            curr_status = Status.DRAW\n",
    "        return curr_status\n",
    "\n",
    "\n",
    "    def play(self, pos:int) -> None:\n",
    "        \"\"\"Plays a move in the board. pos is the position in the magic square corresponding to the move to be played. Returns True if the move was played, False otherwise.\"\"\"\n",
    "        if pos not in self.available:\n",
    "            return False\n",
    "        if len(self.available) % 2 == 1:\n",
    "            self.x.add(pos)\n",
    "        else:\n",
    "            self.o.add(pos)\n",
    "        self.available.remove(pos)\n",
    "        return True\n",
    "\n",
    "\n",
    "    def transform(self, sequence, revert=False):\n",
    "        \"\"\"Applies a sequence of transformations, defined as dictionaries {from: to} to the board. revert = True applies the inverse of the transformations in reverse order.\"\"\"\n",
    "        def apply_transformation(board:TicTacToe, map):\n",
    "            board.available = {map[element] for element in board.available}\n",
    "            board.x = {map[element] for element in board.x}\n",
    "            board.o = {map[element] for element in board.o}\n",
    "            \n",
    "        new = deepcopy(self)\n",
    "        if revert:\n",
    "            sequence = reversed(sequence)\n",
    "        for transformation in sequence:\n",
    "            if revert:\n",
    "                transformation = dict([(value, key) for key, value in transformation.items()])\n",
    "            apply_transformation(new, transformation)\n",
    "        return new\n",
    "\n",
    "\n",
    "    def canonize(self):\n",
    "        \"\"\"Return a canonical state equivalent to the current one and the sequence of transformations used to reach it\"\"\"\n",
    "        rotate = {  # clockwise 90Â° rotation of the board\n",
    "            1: 7,\n",
    "            2: 4,\n",
    "            3: 1,\n",
    "            4: 8,\n",
    "            5: 5,\n",
    "            6: 2,\n",
    "            7: 9,\n",
    "            8: 6,\n",
    "            9: 3\n",
    "        }\n",
    "        flip = {    # vertical flip\n",
    "            1: 9,\n",
    "            2: 6,\n",
    "            3: 3,\n",
    "            4: 8,\n",
    "            5: 5,\n",
    "            6: 2,\n",
    "            7: 7,\n",
    "            8: 4,\n",
    "            9: 1\n",
    "        }\n",
    "        equivalent = {self: list()}  # states that are equivalent to the current and the transformations used to reach them\n",
    "        representations = {int(self)} # set containing the representation of each state\n",
    "        morphs = [[flip], [rotate], [flip, rotate], [rotate, rotate], [flip, rotate, rotate], [rotate, rotate, rotate], [flip, rotate, rotate, rotate]]\n",
    "        for morph in morphs:\n",
    "            result = self.transform(morph)\n",
    "            if int(result) not in representations:\n",
    "                representations.add(int(result))\n",
    "                equivalent[result] = morph\n",
    "        return min(equivalent.items(), key = lambda e: int(e[0]))\n",
    "\n",
    "\n",
    "    def valid(self) -> bool:\n",
    "        \"\"\"Returns False if the state is invalid, True otherwise. A state is invalid if the difference between the number of x and os is greater than one, or if both players won\"\"\"\n",
    "        return len(self.x) - len(self.o) <= 1 and self.check_status() != Status.INVALID\n",
    "\n",
    "\n",
    "    def current_player(self) -> Player:\n",
    "        \"\"\"Returns the player that needs to make a move\"\"\"\n",
    "        return Player.X if len(self.available) % 2 == 1 else Player.O\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a random agent as an agent that always plays a random move for those available.  \n",
    "`random_agent` is a function that, given a `TicTacToe` board, returns an integer representing a move, taken randomly from the available positions in the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "---\n",
      "-x-\n",
      "\n",
      "-o-\n",
      "---\n",
      "-x-\n",
      "\n",
      "-o-\n",
      "-x-\n",
      "-x-\n",
      "\n",
      "-o-\n",
      "-xo\n",
      "-x-\n",
      "\n",
      "-o-\n",
      "-xo\n",
      "xx-\n",
      "\n",
      "oo-\n",
      "-xo\n",
      "xx-\n",
      "\n",
      "oox\n",
      "-xo\n",
      "xx-\n",
      "\n",
      "X_WINS\n"
     ]
    }
   ],
   "source": [
    "def random_agent(board:TicTacToe):\n",
    "    \"\"\"Random agent: plays a random move from those available in the board\"\"\"\n",
    "    return choice(list(board.available))\n",
    "\n",
    "ttt = TicTacToe()\n",
    "status = Status.ONGOING\n",
    "while status == Status.ONGOING:\n",
    "    canon, morph = ttt.canonize()\n",
    "    canon.play(random_agent(canon))\n",
    "    ttt = canon.transform(morph, revert=True)\n",
    "    print(ttt)\n",
    "    status = ttt.check_status()\n",
    "print(Status(status).name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Markov Decision Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This problem can be modeled as a **Markov Decision Process (MDP)**. An MDP consists in\n",
    "- A set of **states**\n",
    "- A set of **actions** for each state, e.g the available moves for each state\n",
    "- A **transition model**, that tells us how performing an action changes the state\n",
    "- A **reward function**, that calculates the reward to give the agent for a given transition from one state to another\n",
    "- A **discount** hyperparameter, that influences how much our agent prefers instant rewards over delayed ones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MDP` class is used to represent the Markov Decision Process.  \n",
    "It contains the following attributes:\n",
    "- `player`: is the player for which we are calculating the MDP\n",
    "- `states`: is the set of board states\n",
    "- `discount`: is the discount hyperparameter\n",
    "\n",
    "It implements the following methods:\n",
    "- `__init__`: initializes the MDP for the player\n",
    "- `generate_states`: populates the states\n",
    "    - It starts by calculating all the possible sequences of moves possible in Tic-Tac-Toe where the `player` has to make a move\n",
    "    - States that are equivalent to each other are then pruned\n",
    "    - Symmetry is then applied to only store the \"canonical\" form of each state\n",
    "    - Using this pruning and symmetry, it goes from 426458 to 426 states for X, from 197074 to 383 states for O\n",
    "- `reward`: it's the reward function\n",
    "    It returns `0` if the state is non-terminal, `1` for a win, `-1` for a loss or invalid state, `-0.5` for a draw\n",
    "- `transition_model`: calculates the probability of reaching a certain state after applying an action to a state\n",
    "    It returns a list of tuples (state, prob) describing the probability of reaching a certain state after applying an action to the state\n",
    "- `q_value`: calculates the quality of an action for a given state using the following equation:\n",
    "$$Q(s, a, U)=\\sum_{s'}P(s'|s, a)[R(s, a, s')+ \\gamma U(s')]$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MDP:\n",
    "    def __init__(self, player = Player.X) -> None:\n",
    "        self.player = player\n",
    "        self.states = self.generate_states()\n",
    "        self.discount = 0.5\n",
    "\n",
    "\n",
    "    def generate_states(self):\n",
    "        \"\"\"Generates all possible states of a Tic-Tac-Toe board where it's the player's turn, applying symmetry and pruning to reduce them.\"\"\"\n",
    "        states = set()\n",
    "#        count = 1\n",
    "        start = 0 if self.player == Player.X else 1\n",
    "        for length in range(start, 9, 2):\n",
    "            perms = list(permutations(range(1, 10), length))\n",
    "#            count += len(perms)\n",
    "            for perm in unique_everseen(perms, key = lambda e: int(TicTacToe(list(e)))):  # remove equivalent boards\n",
    "                ttt, _ = TicTacToe(list(perm)).canonize()\n",
    "                if ttt.valid():\n",
    "                    states.add(int(ttt))\n",
    "#        print(\"Generated\", count, \"states,\", len(states), \"after pruning\")\n",
    "        return states\n",
    "\n",
    "\n",
    "    def reward(self, state):\n",
    "        \"\"\"Calculates the reward for the current state. Returns 0 if the state is non-terminal, 1 if the player wins, -1 if it loses, -0.5 if it's a draw\"\"\"\n",
    "        status = TicTacToe(state).check_status()\n",
    "        if status == Status.ONGOING:\n",
    "            return 0\n",
    "        elif status == Status.DRAW:\n",
    "            return -0.5\n",
    "        elif (status == Status.X_WINS and self.player == Player.X) or (status == Status.O_WINS and self.player == Player.O):\n",
    "            return 1\n",
    "        return -1\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def transition_model(state, action):\n",
    "        \"\"\"Returns a list of tuples (state, prob) describing the probability of reaching a certain state after applying an action to the state\"\"\"\n",
    "        ttt = TicTacToe(state)\n",
    "        if ttt.check_status() != Status.ONGOING: # the state is terminal before the player moves\n",
    "            return [(int(ttt), 1)]\n",
    "        ttt.play(action)\n",
    "        if ttt.check_status() != Status.ONGOING: # the state is terminal after the player moves, but before the opponent does\n",
    "            return [(int(ttt.canonize()[0]), 1)]\n",
    "        probs = {}\n",
    "        for move in ttt.available:\n",
    "            tmp = deepcopy(ttt)\n",
    "            tmp.play(move)\n",
    "            tmp, _ = tmp.canonize()\n",
    "            if tmp.valid():\n",
    "                if int(tmp) not in probs.keys():\n",
    "                    probs[int(tmp)] = 0\n",
    "                probs[int(tmp)] += 1\n",
    "        return [(k, v/sum(probs.values())) for k, v in probs.items()]\n",
    "\n",
    "\n",
    "    def q_value(self, state, action, utilities):\n",
    "        \"\"\"Returns an utility value for the given action at the given state\"\"\"\n",
    "        possible_states = MDP.transition_model(state, action)\n",
    "        value = 0\n",
    "        for possible_state, probability in possible_states:\n",
    "            if TicTacToe(possible_state).check_status() != Status.ONGOING:\n",
    "                value += self.reward(possible_state)\n",
    "            else:\n",
    "                value += probability * (self.reward(possible_state) + self.discount * utilities[possible_state])\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value Iteration\n",
    "We can use the **value iteration** algorithm to solve the MDP, by iteratively calculating the utilities until convergence is reached.\n",
    "The utilities are calculated in the `bellman_update` function, that calculates the **Bellman Equation** for each state.\n",
    "#### Bellman Equation\n",
    "The Bellman Equation is defined as follows:\n",
    "$$U(s)=\\max_{a \\in A(s)}\\sum_{s'}P(s'|s, a)[R(s, a, s') + \\gamma U(s')]$$\n",
    "where:\n",
    "- $a \\in A(s)$ are the possible actions in state $s$\n",
    "- $s'$ is one of the possible states generated by each action\n",
    "- $P(s'| s, a)$ is the probability of reaching state $s'$ by applying the action $a$ to the state $s$\n",
    "- $R(s, a, s')$ is the reward calculated on the transition.\n",
    "- $\\gamma$ is the discount hyperparameter\n",
    "- $U(s')$ is the utility of state $s'$\n",
    "\n",
    "At each iteration the utiliy of each state becomes more accurate, until convergence is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(mdp: MDP, eps = 0.0001):\n",
    "    \"\"\"Calculates the utility of each state. Returns a dictionary containing the best move for each state (policy).\"\"\"\n",
    "    def bellman_update(state, utilities):\n",
    "        res = list()\n",
    "        for action in TicTacToe(state).available:\n",
    "            res.append((action, mdp.q_value(state, action, utilities)))\n",
    "        if len(res) == 0:\n",
    "            return [(None, mdp.reward(state))]\n",
    "        return max(res, key = lambda e: e[1])\n",
    "    \n",
    "    utilities_prime = {state: 0 for state in mdp.states}\n",
    "    policy = {state: None for state in mdp.states}\n",
    "    epoch = 0\n",
    "    while True:\n",
    "        delta = 0\n",
    "        epoch += 1\n",
    "        utilities = utilities_prime.copy()\n",
    "        for state in tqdm(mdp.states):\n",
    "            policy[state], utilities_prime[state] = bellman_update(state, utilities)\n",
    "            delta = max(delta, abs(utilities_prime[state] - utilities[state]))\n",
    "        print(\"epoch\", epoch, \"delta =\", delta)\n",
    "        if delta <= eps * (1 - mdp.discount)/mdp.discount:\n",
    "            break\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 426/426 [00:00<00:00, 453.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 delta = 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 426/426 [00:00<00:00, 450.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 delta = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 426/426 [00:00<00:00, 447.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 delta = 0.25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 426/426 [00:00<00:00, 455.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 delta = 0.020182291666666685\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 426/426 [00:00<00:00, 455.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 delta = 0.00016276041666665741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 426/426 [00:00<00:00, 434.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 6 delta = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "policy_x = value_iteration(MDP(player=Player.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 383/383 [00:00<00:00, 447.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 delta = 2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 383/383 [00:00<00:00, 445.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 2 delta = 0.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 383/383 [00:00<00:00, 444.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3 delta = 0.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 383/383 [00:00<00:00, 441.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4 delta = 0.01428571428571429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 383/383 [00:00<00:00, 444.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 5 delta = 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "policy_o = value_iteration(MDP(player=Player.O))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a `policy_agent` as an agent that plays a move based on a `policy` generated via value or policy iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_agent(policy, board:TicTacToe):\n",
    "    \"\"\"An agent that plays a move based on a policy\"\"\"\n",
    "    return policy[int(board)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(player, policy):\n",
    "    \"\"\"Plays 10000 games against a random agent and prints Win/Loss/Tie rate for the policy and the player\"\"\"\n",
    "    wins = 0\n",
    "    losses = 0\n",
    "    ties = 0\n",
    "    for _ in tqdm(range(100_000)):\n",
    "        ttt = TicTacToe()\n",
    "        status = Status.ONGOING\n",
    "        while status == Status.ONGOING:\n",
    "            if ttt.current_player() == player:\n",
    "                ttt, morph = ttt.canonize()\n",
    "                ttt.play(policy_agent(policy, ttt))\n",
    "                ttt = ttt.transform(morph, revert=True)\n",
    "            else:\n",
    "                ttt.play(random_agent(ttt))\n",
    "            status = ttt.check_status()\n",
    "        if (status == Status.X_WINS and player == Player.X) or (status == Status.O_WINS and player == player.O):\n",
    "            wins += 1\n",
    "        elif (status == Status.O_WINS and player == Player.X) or (status == Status.X_WINS and player == player.O):\n",
    "            losses += 1\n",
    "        else:\n",
    "            ties += 1\n",
    "    print(\"Playing as\", Player(player).name)\n",
    "    print(\"Win rate:\", wins/1_000, \"%\\nLoss rate:\", losses/1_000, \"%\\nTie rate:\", ties/1_000,\"%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100000/100000 [01:17<00:00, 1297.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing as X\n",
      "Win rate: 99.499 %\n",
      "Loss rate: 0.0 %\n",
      "Tie rate: 0.501 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100000/100000 [01:19<00:00, 1258.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing as O\n",
      "Win rate: 91.522 %\n",
      "Loss rate: 0.0 %\n",
      "Tie rate: 8.478 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Iteration\")\n",
    "evaluate_policy(Player.X, policy_x)\n",
    "print()\n",
    "evaluate_policy(Player.O, policy_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy Iteration\n",
    "The MDP can also be solved by **policy iteration**, an algorithm that iteratively calculates the policy by alternating two steps\n",
    "- A **policy evaluation**, that calculates the utility $U_i(s)$ of each state if the policy $\\pi_i$ were to be executed, using a simplified version of the Bellman equation:\n",
    "$$U_i(s) = \\sum_{s'} P(s'| s, \\pi_i(s))[R(s, \\pi_i(s), s')+\\gamma U_i(s')]$$\n",
    "- A **policy improvement**, calculates a new policy $\\pi_{i+1}$ using the following equation:\n",
    "$$\\pi_{i+1}(s) = \\arg\\max_{a \\in A(s)}\\sum_{s'} P(s'| s, a)[R(s, a, s')+\\gamma U_i(s')]$$\n",
    "\n",
    "The policy is randomly initialized, and the iteration stops when the policy stops changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_iteration(mdp: MDP):\n",
    "    \"\"\"Iteratively calculates the policy\"\"\"\n",
    "    def policy_eval(policy, utilities): # policy evaluation\n",
    "        for state in utilities.keys():\n",
    "            utilities[state] = mdp.q_value(state, policy[state], utilities)\n",
    "        return utilities\n",
    "    policy = {state: choice(list(TicTacToe(state).available)) for state in mdp.states}\n",
    "    utilities = {state: 0 for state in mdp.states}\n",
    "    epoch = 0\n",
    "    change = True\n",
    "    while change:\n",
    "        epoch += 1\n",
    "        utilities = policy_eval(policy, utilities)\n",
    "        change = False\n",
    "        for state in tqdm(mdp.states):\n",
    "            best_action = None\n",
    "            best_value = None\n",
    "            for action in TicTacToe(state).available:  # policy improvement\n",
    "                value = mdp.q_value(state, action, utilities)\n",
    "                if best_value is None or best_value < value:\n",
    "                    best_value = value\n",
    "                    best_action = action\n",
    "            if best_value > mdp.q_value(state, policy[state], utilities):\n",
    "                policy[state] = best_action\n",
    "                change = True\n",
    "    print(\"Policy found after\", epoch, \"epochs.\")\n",
    "    return policy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 426/426 [00:01<00:00, 350.36it/s]\n",
      "100%|ââââââââââ| 426/426 [00:01<00:00, 391.71it/s]\n",
      "100%|ââââââââââ| 426/426 [00:01<00:00, 384.66it/s]\n",
      "100%|ââââââââââ| 426/426 [00:01<00:00, 384.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy found after 4 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "policy_x = policy_iteration(mdp=MDP(player=Player.X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 383/383 [00:01<00:00, 366.43it/s]\n",
      "100%|ââââââââââ| 383/383 [00:01<00:00, 373.63it/s]\n",
      "100%|ââââââââââ| 383/383 [00:01<00:00, 369.36it/s]\n",
      "100%|ââââââââââ| 383/383 [00:01<00:00, 363.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy found after 4 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "policy_o = policy_iteration(mdp=MDP(player=Player.O))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100000/100000 [01:16<00:00, 1304.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing as X\n",
      "Win rate: 99.458 %\n",
      "Loss rate: 0.0 %\n",
      "Tie rate: 0.542 %\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100000/100000 [01:18<00:00, 1274.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing as O\n",
      "Win rate: 91.626 %\n",
      "Loss rate: 0.0 %\n",
      "Tie rate: 8.374 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Policy Iteration\")\n",
    "evaluate_policy(Player.X, policy_x)\n",
    "print()\n",
    "evaluate_policy(Player.O, policy_o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For playing Tic-Tac-Toe against a random opponent, value and policy iteration provide comparable results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci-fLJ3OwGs-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
